[2024-08-17T19:52:00.952+0100] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-08-17T19:52:00.980+0100] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: jobs_etl.linkedIn_scraper scheduled__2024-08-16T18:00:00+00:00 [queued]>
[2024-08-17T19:52:00.987+0100] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: jobs_etl.linkedIn_scraper scheduled__2024-08-16T18:00:00+00:00 [queued]>
[2024-08-17T19:52:00.987+0100] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-08-17T19:52:01.008+0100] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): linkedIn_scraper> on 2024-08-16 18:00:00+00:00
[2024-08-17T19:52:01.023+0100] {standard_task_runner.py:63} INFO - Started process 33672 to run task
[2024-08-17T19:52:01.028+0100] {standard_task_runner.py:90} INFO - Running: ['airflow', 'tasks', 'run', 'jobs_etl', 'linkedIn_scraper', 'scheduled__2024-08-16T18:00:00+00:00', '--job-id', '398', '--raw', '--subdir', 'DAGS_FOLDER/jobs_etl.py', '--cfg-path', '/tmp/tmpmyjlvkfj']
[2024-08-17T19:52:01.030+0100] {standard_task_runner.py:91} INFO - Job 398: Subtask linkedIn_scraper
[2024-08-17T19:52:01.096+0100] {task_command.py:426} INFO - Running <TaskInstance: jobs_etl.linkedIn_scraper scheduled__2024-08-16T18:00:00+00:00 [running]> on host huser
[2024-08-17T19:52:01.191+0100] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_EMAIL='hassounibarka@gmail.com' AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='jobs_etl' AIRFLOW_CTX_TASK_ID='linkedIn_scraper' AIRFLOW_CTX_EXECUTION_DATE='2024-08-16T18:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-08-16T18:00:00+00:00'
[2024-08-17T19:52:01.191+0100] {taskinstance.py:430} INFO - ::endgroup::
[2024-08-17T19:52:49.995+0100] {logging_mixin.py:188} INFO - number_of_jobs: 19
[2024-08-17T19:52:49.996+0100] {logging_mixin.py:188} INFO - number_of_pages: 1
[2024-08-17T19:52:49.996+0100] {logging_mixin.py:188} INFO - Scraping page: 1
[2024-08-17T19:52:49.998+0100] {logging_mixin.py:188} INFO - done!
[2024-08-17T19:53:47.259+0100] {logging_mixin.py:188} INFO - 1 Jobs read2 Jobs read3 Jobs read4 Jobs read5 Jobs read6 Jobs read7 Jobs read8 Jobs read9 Jobs read10 Jobs read11 Jobs read12 Jobs read13 Jobs read14 Jobs read15 Jobs read16 Jobs read17 Jobs read18 Jobs read19 Jobs readnumber_of_jobs: 0
[2024-08-17T19:53:47.259+0100] {logging_mixin.py:188} INFO - number_of_pages: 0
[2024-08-17T19:54:06.418+0100] {logging_mixin.py:188} INFO - number_of_jobs: 14
[2024-08-17T19:54:06.419+0100] {logging_mixin.py:188} INFO - number_of_pages: 1
[2024-08-17T19:54:06.419+0100] {logging_mixin.py:188} INFO - Scraping page: 1
[2024-08-17T19:54:06.421+0100] {logging_mixin.py:188} INFO - done!
[2024-08-17T19:54:57.233+0100] {logging_mixin.py:188} INFO - 1 Jobs read2 Jobs read3 Jobs read4 Jobs read5 Jobs read6 Jobs read7 Jobs read8 Jobs read9 Jobs read10 Jobs read11 Jobs read12 Jobs read13 Jobs read14 Jobs readnumber_of_jobs: 0
[2024-08-17T19:54:57.234+0100] {logging_mixin.py:188} INFO - number_of_pages: 0
[2024-08-17T19:55:09.470+0100] {logging_mixin.py:188} INFO - number_of_jobs: 13
[2024-08-17T19:55:09.470+0100] {logging_mixin.py:188} INFO - number_of_pages: 1
[2024-08-17T19:55:09.470+0100] {logging_mixin.py:188} INFO - Scraping page: 1
[2024-08-17T19:55:09.472+0100] {logging_mixin.py:188} INFO - done!
[2024-08-17T19:56:02.138+0100] {logging_mixin.py:188} INFO - 1 Jobs read2 Jobs read3 Jobs read4 Jobs read5 Jobs read6 Jobs read7 Jobs read8 Jobs read9 Jobs read10 Jobs read11 Jobs read12 Jobs read13 Jobs readnumber_of_jobs: 0
[2024-08-17T19:56:02.139+0100] {logging_mixin.py:188} INFO - number_of_pages: 0
[2024-08-17T19:56:16.949+0100] {logging_mixin.py:188} INFO - number_of_jobs: 6
[2024-08-17T19:56:16.950+0100] {logging_mixin.py:188} INFO - number_of_pages: 1
[2024-08-17T19:56:16.950+0100] {logging_mixin.py:188} INFO - Scraping page: 1
[2024-08-17T19:56:16.951+0100] {logging_mixin.py:188} INFO - done!
[2024-08-17T19:56:44.744+0100] {logging_mixin.py:188} INFO - 1 Jobs read2 Jobs read3 Jobs read4 Jobs read5 Jobs read6 Jobs readnumber_of_jobs: 0
[2024-08-17T19:56:44.744+0100] {logging_mixin.py:188} INFO - number_of_pages: 0
[2024-08-17T19:56:52.622+0100] {logging_mixin.py:188} INFO - number_of_jobs: 5
[2024-08-17T19:56:52.622+0100] {logging_mixin.py:188} INFO - number_of_pages: 1
[2024-08-17T19:56:52.623+0100] {logging_mixin.py:188} INFO - Scraping page: 1
[2024-08-17T19:56:52.624+0100] {logging_mixin.py:188} INFO - done!
[2024-08-17T19:57:13.846+0100] {logging_mixin.py:188} INFO - 1 Jobs read2 Jobs read3 Jobs read4 Jobs read5 Jobs readnumber_of_jobs: 0
[2024-08-17T19:57:13.846+0100] {logging_mixin.py:188} INFO - number_of_pages: 0
[2024-08-17T19:57:13.851+0100] {logging_mixin.py:188} INFO - Done!
[2024-08-17T19:57:13.940+0100] {python.py:237} INFO - Done. Returned value was: None
[2024-08-17T19:57:13.940+0100] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-08-17T19:57:13.957+0100] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=jobs_etl, task_id=linkedIn_scraper, run_id=scheduled__2024-08-16T18:00:00+00:00, execution_date=20240816T180000, start_date=20240817T185200, end_date=20240817T185713
[2024-08-17T19:57:14.014+0100] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-08-17T19:57:14.044+0100] {taskinstance.py:3498} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-08-17T19:57:14.045+0100] {local_task_job_runner.py:222} INFO - ::endgroup::
