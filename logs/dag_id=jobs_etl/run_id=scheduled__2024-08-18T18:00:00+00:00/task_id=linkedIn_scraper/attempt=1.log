[2024-08-19T23:07:00.625+0100] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-08-19T23:07:00.658+0100] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: jobs_etl.linkedIn_scraper scheduled__2024-08-18T18:00:00+00:00 [queued]>
[2024-08-19T23:07:00.666+0100] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: jobs_etl.linkedIn_scraper scheduled__2024-08-18T18:00:00+00:00 [queued]>
[2024-08-19T23:07:00.667+0100] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-08-19T23:07:00.678+0100] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): linkedIn_scraper> on 2024-08-18 18:00:00+00:00
[2024-08-19T23:07:00.690+0100] {standard_task_runner.py:63} INFO - Started process 83149 to run task
[2024-08-19T23:07:00.698+0100] {standard_task_runner.py:90} INFO - Running: ['airflow', 'tasks', 'run', 'jobs_etl', 'linkedIn_scraper', 'scheduled__2024-08-18T18:00:00+00:00', '--job-id', '407', '--raw', '--subdir', 'DAGS_FOLDER/jobs_etl.py', '--cfg-path', '/tmp/tmpo6o32edl']
[2024-08-19T23:07:00.701+0100] {standard_task_runner.py:91} INFO - Job 407: Subtask linkedIn_scraper
[2024-08-19T23:07:00.780+0100] {task_command.py:426} INFO - Running <TaskInstance: jobs_etl.linkedIn_scraper scheduled__2024-08-18T18:00:00+00:00 [running]> on host huser
[2024-08-19T23:07:00.902+0100] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_EMAIL='hassounibarka@gmail.com' AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='jobs_etl' AIRFLOW_CTX_TASK_ID='linkedIn_scraper' AIRFLOW_CTX_EXECUTION_DATE='2024-08-18T18:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-08-18T18:00:00+00:00'
[2024-08-19T23:07:00.903+0100] {taskinstance.py:430} INFO - ::endgroup::
[2024-08-19T23:07:53.691+0100] {logging_mixin.py:188} INFO - number_of_jobs: 26
[2024-08-19T23:07:53.692+0100] {logging_mixin.py:188} INFO - number_of_pages: 2
[2024-08-19T23:07:53.692+0100] {logging_mixin.py:188} INFO - Scraping page: 1
[2024-08-19T23:07:53.696+0100] {logging_mixin.py:188} INFO - done!
[2024-08-19T23:07:53.696+0100] {logging_mixin.py:188} INFO - Scraping page: 2
[2024-08-19T23:08:18.464+0100] {logging_mixin.py:188} INFO - done!
[2024-08-19T23:11:18.627+0100] {logging_mixin.py:188} INFO - 1 Jobs read2 Jobs read3 Jobs read4 Jobs read5 Jobs read6 Jobs read7 Jobs read8 Jobs read9 Jobs read10 Jobs read11 Jobs read12 Jobs read13 Jobs read14 Jobs read15 Jobs read16 Jobs read17 Jobs read18 Jobs read19 Jobs read20 Jobs read21 Jobs read22 Jobs read23 Jobs read24 Jobs read25 Jobs read26 Jobs readnumber_of_jobs: 0
[2024-08-19T23:11:18.627+0100] {logging_mixin.py:188} INFO - number_of_pages: 0
[2024-08-19T23:11:28.847+0100] {logging_mixin.py:188} INFO - number_of_jobs: 35
[2024-08-19T23:11:28.848+0100] {logging_mixin.py:188} INFO - number_of_pages: 2
[2024-08-19T23:11:28.848+0100] {logging_mixin.py:188} INFO - Scraping page: 1
[2024-08-19T23:11:28.852+0100] {logging_mixin.py:188} INFO - done!
[2024-08-19T23:11:28.852+0100] {logging_mixin.py:188} INFO - Scraping page: 2
[2024-08-19T23:11:36.556+0100] {logging_mixin.py:188} INFO - done!
[2024-08-19T23:15:19.007+0100] {logging_mixin.py:188} INFO - 1 Jobs read2 Jobs read3 Jobs read4 Jobs read5 Jobs read6 Jobs read7 Jobs read8 Jobs read9 Jobs read10 Jobs read11 Jobs read12 Jobs read13 Jobs read14 Jobs read15 Jobs read16 Jobs read17 Jobs read18 Jobs read19 Jobs read20 Jobs read21 Jobs read22 Jobs read23 Jobs read24 Jobs read25 Jobs read26 Jobs read27 Jobs read28 Jobs read29 Jobs read30 Jobs read31 Jobs read32 Jobs readnumber_of_jobs: 1
[2024-08-19T23:15:19.008+0100] {logging_mixin.py:188} INFO - number_of_pages: 1
[2024-08-19T23:15:19.008+0100] {logging_mixin.py:188} INFO - Scraping page: 1
[2024-08-19T23:15:19.009+0100] {logging_mixin.py:188} INFO - done!
[2024-08-19T23:15:44.060+0100] {logging_mixin.py:188} INFO - 1 Jobs readnumber_of_jobs: 45
[2024-08-19T23:15:44.060+0100] {logging_mixin.py:188} INFO - number_of_pages: 2
[2024-08-19T23:15:44.060+0100] {logging_mixin.py:188} INFO - Scraping page: 1
[2024-08-19T23:15:44.062+0100] {logging_mixin.py:188} INFO - done!
[2024-08-19T23:15:44.062+0100] {logging_mixin.py:188} INFO - Scraping page: 2
[2024-08-19T23:16:17.093+0100] {logging_mixin.py:188} INFO - done!
[2024-08-19T23:21:33.145+0100] {logging_mixin.py:188} INFO - 1 Jobs read2 Jobs read3 Jobs read4 Jobs read5 Jobs read6 Jobs read7 Jobs read8 Jobs read9 Jobs read10 Jobs read11 Jobs read12 Jobs read13 Jobs read14 Jobs read15 Jobs read16 Jobs read17 Jobs read18 Jobs read19 Jobs read20 Jobs read21 Jobs read22 Jobs read23 Jobs read24 Jobs read25 Jobs read26 Jobs read27 Jobs read28 Jobs read29 Jobs read30 Jobs read31 Jobs read32 Jobs read33 Jobs read34 Jobs read35 Jobs read36 Jobs read37 Jobs read38 Jobs read39 Jobs read40 Jobs read41 Jobs read42 Jobs read43 Jobs read44 Jobs read45 Jobs readnumber_of_jobs: 3
[2024-08-19T23:21:33.146+0100] {logging_mixin.py:188} INFO - number_of_pages: 1
[2024-08-19T23:21:33.146+0100] {logging_mixin.py:188} INFO - Scraping page: 1
[2024-08-19T23:21:33.148+0100] {logging_mixin.py:188} INFO - done!
[2024-08-19T23:22:01.658+0100] {logging_mixin.py:188} INFO - 1 Jobs read2 Jobs read3 Jobs readnumber_of_jobs: 9
[2024-08-19T23:22:01.659+0100] {logging_mixin.py:188} INFO - number_of_pages: 1
[2024-08-19T23:22:01.659+0100] {logging_mixin.py:188} INFO - Scraping page: 1
[2024-08-19T23:22:01.661+0100] {logging_mixin.py:188} INFO - done!
[2024-08-19T23:22:58.197+0100] {logging_mixin.py:188} INFO - 1 Jobs read2 Jobs read3 Jobs read4 Jobs read5 Jobs read6 Jobs read7 Jobs read8 Jobs read9 Jobs readnumber_of_jobs: 1
[2024-08-19T23:22:58.197+0100] {logging_mixin.py:188} INFO - number_of_pages: 1
[2024-08-19T23:22:58.197+0100] {logging_mixin.py:188} INFO - Scraping page: 1
[2024-08-19T23:22:58.199+0100] {logging_mixin.py:188} INFO - done!
[2024-08-19T23:23:10.303+0100] {logging_mixin.py:188} INFO - 1 Jobs readnumber_of_jobs: 6
[2024-08-19T23:23:10.304+0100] {logging_mixin.py:188} INFO - number_of_pages: 1
[2024-08-19T23:23:10.304+0100] {logging_mixin.py:188} INFO - Scraping page: 1
[2024-08-19T23:23:10.306+0100] {logging_mixin.py:188} INFO - done!
[2024-08-19T23:24:07.385+0100] {logging_mixin.py:188} INFO - 1 Jobs read2 Jobs read3 Jobs read4 Jobs read5 Jobs read6 Jobs readnumber_of_jobs: 1
[2024-08-19T23:24:07.386+0100] {logging_mixin.py:188} INFO - number_of_pages: 1
[2024-08-19T23:24:07.386+0100] {logging_mixin.py:188} INFO - Scraping page: 1
[2024-08-19T23:24:07.387+0100] {logging_mixin.py:188} INFO - done!
[2024-08-19T23:24:12.267+0100] {logging_mixin.py:188} INFO - 1 Jobs readDone!
[2024-08-19T23:24:12.326+0100] {python.py:237} INFO - Done. Returned value was: None
[2024-08-19T23:24:12.327+0100] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-08-19T23:24:12.349+0100] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=jobs_etl, task_id=linkedIn_scraper, run_id=scheduled__2024-08-18T18:00:00+00:00, execution_date=20240818T180000, start_date=20240819T220700, end_date=20240819T222412
[2024-08-19T23:24:12.388+0100] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-08-19T23:24:12.434+0100] {taskinstance.py:3498} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-08-19T23:24:12.435+0100] {local_task_job_runner.py:222} INFO - ::endgroup::
