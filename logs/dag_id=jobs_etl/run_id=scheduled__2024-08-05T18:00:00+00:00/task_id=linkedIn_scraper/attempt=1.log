[2024-08-06T23:35:54.567+0100] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-08-06T23:35:54.600+0100] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: jobs_etl.linkedIn_scraper scheduled__2024-08-05T18:00:00+00:00 [queued]>
[2024-08-06T23:35:54.606+0100] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: jobs_etl.linkedIn_scraper scheduled__2024-08-05T18:00:00+00:00 [queued]>
[2024-08-06T23:35:54.606+0100] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-08-06T23:35:54.614+0100] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): linkedIn_scraper> on 2024-08-05 18:00:00+00:00
[2024-08-06T23:35:54.622+0100] {standard_task_runner.py:63} INFO - Started process 698168 to run task
[2024-08-06T23:35:54.626+0100] {standard_task_runner.py:90} INFO - Running: ['airflow', 'tasks', 'run', 'jobs_etl', 'linkedIn_scraper', 'scheduled__2024-08-05T18:00:00+00:00', '--job-id', '307', '--raw', '--subdir', 'DAGS_FOLDER/jobs_etl.py', '--cfg-path', '/tmp/tmpy52jgwvk']
[2024-08-06T23:35:54.628+0100] {standard_task_runner.py:91} INFO - Job 307: Subtask linkedIn_scraper
[2024-08-06T23:35:54.698+0100] {task_command.py:426} INFO - Running <TaskInstance: jobs_etl.linkedIn_scraper scheduled__2024-08-05T18:00:00+00:00 [running]> on host huser
[2024-08-06T23:35:54.810+0100] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_EMAIL='hassounibarka@gmail.com' AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='jobs_etl' AIRFLOW_CTX_TASK_ID='linkedIn_scraper' AIRFLOW_CTX_EXECUTION_DATE='2024-08-05T18:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-08-05T18:00:00+00:00'
[2024-08-06T23:35:54.812+0100] {taskinstance.py:430} INFO - ::endgroup::
[2024-08-06T23:37:03.209+0100] {logging_mixin.py:188} INFO - number_of_jobs: 326
[2024-08-06T23:37:03.210+0100] {logging_mixin.py:188} INFO - number_of_pages: 14
[2024-08-06T23:37:03.210+0100] {logging_mixin.py:188} INFO - Scraping page: 1
[2024-08-06T23:37:03.213+0100] {logging_mixin.py:188} INFO - done!
[2024-08-06T23:37:03.213+0100] {logging_mixin.py:188} INFO - Scraping page: 2
[2024-08-06T23:37:20.895+0100] {logging_mixin.py:188} INFO - done!
[2024-08-06T23:37:20.896+0100] {logging_mixin.py:188} INFO - Scraping page: 3
[2024-08-06T23:37:36.820+0100] {logging_mixin.py:188} INFO - done!
[2024-08-06T23:37:36.820+0100] {logging_mixin.py:188} INFO - Scraping page: 4
[2024-08-06T23:38:09.808+0100] {logging_mixin.py:188} INFO - done!
[2024-08-06T23:38:09.809+0100] {logging_mixin.py:188} INFO - Scraping page: 5
[2024-08-06T23:46:39.834+0100] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-08-06T23:46:39.836+0100] {taskinstance.py:2905} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/huser/airflow_workspace/airflow_env/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 465, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/huser/airflow_workspace/airflow_env/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 432, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
  File "/home/huser/airflow_workspace/airflow_env/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 400, in wrapper
    return func(self, *args, **kwargs)
  File "/home/huser/airflow_workspace/airflow_env/lib/python3.10/site-packages/airflow/operators/python.py", line 235, in execute
    return_value = self.execute_callable()
  File "/home/huser/airflow_workspace/airflow_env/lib/python3.10/site-packages/airflow/operators/python.py", line 252, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/home/huser/airflow_workspace/airflow/dags/jobs_etl.py", line 42, in get_linked_data
    df = linked_scraper.get_datas(driver,Keywords,Locations)
  File "/home/huser/airflow_workspace/airflow/dags/Jobs/LinkedScrper.py", line 188, in get_datas
    df = get_data(driver,keyword,location)
  File "/home/huser/airflow_workspace/airflow/dags/Jobs/LinkedScrper.py", line 71, in get_data
    number_of_pages, soup = nb_of_page(driver,url)
  File "/home/huser/airflow_workspace/airflow/dags/Jobs/LinkedScrper.py", line 32, in nb_of_page
    driver.get(url)
  File "/home/huser/airflow_workspace/airflow_env/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 363, in get
    self.execute(Command.GET, {"url": url})
  File "/home/huser/airflow_workspace/airflow_env/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py", line 354, in execute
    self.error_handler.check_response(response)
  File "/home/huser/airflow_workspace/airflow_env/lib/python3.10/site-packages/selenium/webdriver/remote/errorhandler.py", line 229, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.WebDriverException: Message: unknown error: session deleted because of page crash
from unknown error: cannot determine loading status
from tab crashed
  (Session info: chrome=126.0.6478.114)
Stacktrace:
#0 0x5792e905d69a <unknown>
#1 0x5792e8d3ff21 <unknown>
#2 0x5792e8d26fe5 <unknown>
#3 0x5792e8d24f76 <unknown>
#4 0x5792e8d255ef <unknown>
#5 0x5792e8d35f17 <unknown>
#6 0x5792e8d4c46e <unknown>
#7 0x5792e8d51b6b <unknown>
#8 0x5792e8d25d3a <unknown>
#9 0x5792e8d4c026 <unknown>
#10 0x5792e8dce48c <unknown>
#11 0x5792e8daf613 <unknown>
#12 0x5792e8d7f4f7 <unknown>
#13 0x5792e8d7fe4e <unknown>
#14 0x5792e902386b <unknown>
#15 0x5792e9027911 <unknown>
#16 0x5792e900f35e <unknown>
#17 0x5792e9028472 <unknown>
#18 0x5792e8ff3cbf <unknown>
#19 0x5792e904d098 <unknown>
#20 0x5792e904d270 <unknown>
#21 0x5792e905c7cc <unknown>
#22 0x71d126094ac3 <unknown>

[2024-08-06T23:46:39.872+0100] {taskinstance.py:1206} INFO - Marking task as UP_FOR_RETRY. dag_id=jobs_etl, task_id=linkedIn_scraper, run_id=scheduled__2024-08-05T18:00:00+00:00, execution_date=20240805T180000, start_date=20240806T223554, end_date=20240806T224639
[2024-08-06T23:46:39.890+0100] {standard_task_runner.py:110} ERROR - Failed to execute job 307 for task linkedIn_scraper (Message: unknown error: session deleted because of page crash
from unknown error: cannot determine loading status
from tab crashed
  (Session info: chrome=126.0.6478.114)
Stacktrace:
#0 0x5792e905d69a <unknown>
#1 0x5792e8d3ff21 <unknown>
#2 0x5792e8d26fe5 <unknown>
#3 0x5792e8d24f76 <unknown>
#4 0x5792e8d255ef <unknown>
#5 0x5792e8d35f17 <unknown>
#6 0x5792e8d4c46e <unknown>
#7 0x5792e8d51b6b <unknown>
#8 0x5792e8d25d3a <unknown>
#9 0x5792e8d4c026 <unknown>
#10 0x5792e8dce48c <unknown>
#11 0x5792e8daf613 <unknown>
#12 0x5792e8d7f4f7 <unknown>
#13 0x5792e8d7fe4e <unknown>
#14 0x5792e902386b <unknown>
#15 0x5792e9027911 <unknown>
#16 0x5792e900f35e <unknown>
#17 0x5792e9028472 <unknown>
#18 0x5792e8ff3cbf <unknown>
#19 0x5792e904d098 <unknown>
#20 0x5792e904d270 <unknown>
#21 0x5792e905c7cc <unknown>
#22 0x71d126094ac3 <unknown>
; 698168)
[2024-08-06T23:46:39.891+0100] {logging_mixin.py:188} INFO - 1 Jobs read2 Jobs read3 Jobs read4 Jobs read5 Jobs read6 Jobs read7 Jobs read8 Jobs read9 Jobs read10 Jobs read11 Jobs read12 Jobs read13 Jobs read14 Jobs read15 Jobs read16 Jobs read17 Jobs read18 Jobs read19 Jobs read20 Jobs read21 Jobs read22 Jobs read23 Jobs read24 Jobs read25 Jobs read26 Jobs read27 Jobs read28 Jobs read29 Jobs read30 Jobs read31 Jobs read32 Jobs read33 Jobs read34 Jobs read35 Jobs read36 Jobs read37 Jobs read38 Jobs read39 Jobs read40 Jobs read41 Jobs read42 Jobs read43 Jobs read44 Jobs read45 Jobs read46 Jobs read47 Jobs read48 Jobs read49 Jobs read50 Jobs read51 Jobs read52 Jobs read53 Jobs read54 Jobs read55 Jobs read56 Jobs read57 Jobs read58 Jobs read59 Jobs read60 Jobs read61 Jobs read62 Jobs read63 Jobs read64 Jobs read65 Jobs read66 Jobs read67 Jobs read68 Jobs read69 Jobs read70 Jobs read71 Jobs read72 Jobs read73 Jobs read74 Jobs read75 Jobs read76 Jobs read77 Jobs read78 Jobs read79 Jobs read80 Jobs read81 Jobs read82 Jobs read83 Jobs read84 Jobs read85 Jobs read86 Jobs read87 Jobs read88 Jobs read89 Jobs read90 Jobs read91 Jobs read92 Jobs read93 Jobs read94 Jobs read95 Jobs read96 Jobs read97 Jobs read98 Jobs read99 Jobs read100 Jobs read101 Jobs read102 Jobs read103 Jobs read104 Jobs read105 Jobs read106 Jobs read107 Jobs read108 Jobs read109 Jobs read110 Jobs read111 Jobs read112 Jobs read113 Jobs read114 Jobs read115 Jobs read116 Jobs read117 Jobs read118 Jobs read119 Jobs read120 Jobs read121 Jobs read122 Jobs read123 Jobs read124 Jobs read125 Jobs read
[2024-08-06T23:46:39.921+0100] {local_task_job_runner.py:240} INFO - Task exited with return code 1
[2024-08-06T23:46:39.951+0100] {taskinstance.py:3498} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-08-06T23:46:39.952+0100] {local_task_job_runner.py:222} INFO - ::endgroup::
