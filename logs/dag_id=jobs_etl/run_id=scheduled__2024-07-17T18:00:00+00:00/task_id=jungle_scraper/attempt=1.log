[2024-07-18T19:00:02.319+0100] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-07-18T19:00:02.350+0100] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: jobs_etl.jungle_scraper scheduled__2024-07-17T18:00:00+00:00 [queued]>
[2024-07-18T19:00:02.356+0100] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: jobs_etl.jungle_scraper scheduled__2024-07-17T18:00:00+00:00 [queued]>
[2024-07-18T19:00:02.357+0100] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-07-18T19:00:02.366+0100] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): jungle_scraper> on 2024-07-17 18:00:00+00:00
[2024-07-18T19:00:02.371+0100] {standard_task_runner.py:63} INFO - Started process 27194 to run task
[2024-07-18T19:00:02.375+0100] {standard_task_runner.py:90} INFO - Running: ['airflow', 'tasks', 'run', 'jobs_etl', 'jungle_scraper', 'scheduled__2024-07-17T18:00:00+00:00', '--job-id', '159', '--raw', '--subdir', 'DAGS_FOLDER/jobs_etl.py', '--cfg-path', '/tmp/tmp_r81vcis']
[2024-07-18T19:00:02.377+0100] {standard_task_runner.py:91} INFO - Job 159: Subtask jungle_scraper
[2024-07-18T19:00:02.444+0100] {task_command.py:426} INFO - Running <TaskInstance: jobs_etl.jungle_scraper scheduled__2024-07-17T18:00:00+00:00 [running]> on host huser
[2024-07-18T19:00:02.557+0100] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_EMAIL='hassounibarka@gmail.com' AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='jobs_etl' AIRFLOW_CTX_TASK_ID='jungle_scraper' AIRFLOW_CTX_EXECUTION_DATE='2024-07-17T18:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-07-17T18:00:00+00:00'
[2024-07-18T19:00:02.558+0100] {taskinstance.py:430} INFO - ::endgroup::
[2024-07-18T19:00:18.563+0100] {logging_mixin.py:188} INFO - 11 Jobs for data Scientist
[2024-07-18T19:00:18.619+0100] {logging_mixin.py:188} INFO - data Scientist Job extraction ...
[2024-07-18T19:00:44.799+0100] {logging_mixin.py:188} INFO - 0 Jobs read1 Jobs read2 Jobs read3 Jobs read4 Jobs read5 Jobs read6 Jobs read7 Jobs read8 Jobs read9 Jobs read10 Jobs read29 Jobs for data analyst
[2024-07-18T19:00:44.865+0100] {logging_mixin.py:188} INFO - data analyst Job extraction ...
[2024-07-18T19:01:39.140+0100] {logging_mixin.py:188} INFO - 0 Jobs read1 Jobs read2 Jobs read3 Jobs read4 Jobs read5 Jobs read6 Jobs read7 Jobs read8 Jobs read9 Jobs read10 Jobs read11 Jobs read12 Jobs read13 Jobs read14 Jobs read15 Jobs read16 Jobs read17 Jobs read18 Jobs read19 Jobs read20 Jobs read21 Jobs read22 Jobs read23 Jobs read24 Jobs read25 Jobs read26 Jobs read27 Jobs read28 Jobs read22 Jobs for data engineer
[2024-07-18T19:01:39.195+0100] {logging_mixin.py:188} INFO - data engineer Job extraction ...
[2024-07-18T19:10:39.832+0100] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-07-18T19:10:39.833+0100] {taskinstance.py:2905} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/huser/airflow_workspace/airflow_env/lib/python3.10/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
  File "/home/huser/airflow_workspace/airflow_env/lib/python3.10/site-packages/urllib3/util/connection.py", line 85, in create_connection
    raise err
  File "/home/huser/airflow_workspace/airflow_env/lib/python3.10/site-packages/urllib3/util/connection.py", line 73, in create_connection
    sock.connect(sa)
TimeoutError: [Errno 110] Connection timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/huser/airflow_workspace/airflow_env/lib/python3.10/site-packages/urllib3/connectionpool.py", line 793, in urlopen
    response = self._make_request(
  File "/home/huser/airflow_workspace/airflow_env/lib/python3.10/site-packages/urllib3/connectionpool.py", line 491, in _make_request
    raise new_e
  File "/home/huser/airflow_workspace/airflow_env/lib/python3.10/site-packages/urllib3/connectionpool.py", line 467, in _make_request
    self._validate_conn(conn)
  File "/home/huser/airflow_workspace/airflow_env/lib/python3.10/site-packages/urllib3/connectionpool.py", line 1099, in _validate_conn
    conn.connect()
  File "/home/huser/airflow_workspace/airflow_env/lib/python3.10/site-packages/urllib3/connection.py", line 616, in connect
    self.sock = sock = self._new_conn()
  File "/home/huser/airflow_workspace/airflow_env/lib/python3.10/site-packages/urllib3/connection.py", line 207, in _new_conn
    raise ConnectTimeoutError(
urllib3.exceptions.ConnectTimeoutError: (<urllib3.connection.HTTPSConnection object at 0x72dae08ff370>, 'Connection to www.welcometothejungle.com timed out. (connect timeout=None)')

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/huser/airflow_workspace/airflow_env/lib/python3.10/site-packages/requests/adapters.py", line 667, in send
    resp = conn.urlopen(
  File "/home/huser/airflow_workspace/airflow_env/lib/python3.10/site-packages/urllib3/connectionpool.py", line 847, in urlopen
    retries = retries.increment(
  File "/home/huser/airflow_workspace/airflow_env/lib/python3.10/site-packages/urllib3/util/retry.py", line 515, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.welcometothejungle.com', port=443): Max retries exceeded with url: /fr/companies/hinfact/jobs/alternant-e-data-ia_toulouse?q=d128b013a742d70dbf32630dc7073e87&o=e12bab73-75bc-4f48-8a62-820484ade271 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x72dae08ff370>, 'Connection to www.welcometothejungle.com timed out. (connect timeout=None)'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/huser/airflow_workspace/airflow_env/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 465, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/huser/airflow_workspace/airflow_env/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 432, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
  File "/home/huser/airflow_workspace/airflow_env/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 400, in wrapper
    return func(self, *args, **kwargs)
  File "/home/huser/airflow_workspace/airflow_env/lib/python3.10/site-packages/airflow/operators/python.py", line 235, in execute
    return_value = self.execute_callable()
  File "/home/huser/airflow_workspace/airflow_env/lib/python3.10/site-packages/airflow/operators/python.py", line 252, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/home/huser/airflow_workspace/airflow/dags/jobs_etl.py", line 23, in get_jungle_data
    df = jungle_scraper.get_datas(Keywords)
  File "/home/huser/airflow_workspace/airflow/dags/Jobs/JungleScraper.py", line 124, in get_datas
    df = get_data(key)
  File "/home/huser/airflow_workspace/airflow/dags/Jobs/JungleScraper.py", line 57, in get_data
    resp = requests.get(links[i])
  File "/home/huser/airflow_workspace/airflow_env/lib/python3.10/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
  File "/home/huser/airflow_workspace/airflow_env/lib/python3.10/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
  File "/home/huser/airflow_workspace/airflow_env/lib/python3.10/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "/home/huser/airflow_workspace/airflow_env/lib/python3.10/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "/home/huser/airflow_workspace/airflow_env/lib/python3.10/site-packages/requests/adapters.py", line 688, in send
    raise ConnectTimeout(e, request=request)
requests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='www.welcometothejungle.com', port=443): Max retries exceeded with url: /fr/companies/hinfact/jobs/alternant-e-data-ia_toulouse?q=d128b013a742d70dbf32630dc7073e87&o=e12bab73-75bc-4f48-8a62-820484ade271 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x72dae08ff370>, 'Connection to www.welcometothejungle.com timed out. (connect timeout=None)'))
[2024-07-18T19:10:39.868+0100] {taskinstance.py:1206} INFO - Marking task as UP_FOR_RETRY. dag_id=jobs_etl, task_id=jungle_scraper, run_id=scheduled__2024-07-17T18:00:00+00:00, execution_date=20240717T180000, start_date=20240718T180002, end_date=20240718T181039
[2024-07-18T19:10:39.885+0100] {standard_task_runner.py:110} ERROR - Failed to execute job 159 for task jungle_scraper (HTTPSConnectionPool(host='www.welcometothejungle.com', port=443): Max retries exceeded with url: /fr/companies/hinfact/jobs/alternant-e-data-ia_toulouse?q=d128b013a742d70dbf32630dc7073e87&o=e12bab73-75bc-4f48-8a62-820484ade271 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x72dae08ff370>, 'Connection to www.welcometothejungle.com timed out. (connect timeout=None)')); 27194)
[2024-07-18T19:10:39.916+0100] {local_task_job_runner.py:240} INFO - Task exited with return code 1
[2024-07-18T19:10:39.951+0100] {taskinstance.py:3498} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-07-18T19:10:39.953+0100] {local_task_job_runner.py:222} INFO - ::endgroup::
