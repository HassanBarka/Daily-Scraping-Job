[2024-08-04T19:00:05.006+0100] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-08-04T19:00:05.046+0100] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: jobs_etl.jungle_scraper scheduled__2024-08-03T18:00:00+00:00 [queued]>
[2024-08-04T19:00:05.052+0100] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: jobs_etl.jungle_scraper scheduled__2024-08-03T18:00:00+00:00 [queued]>
[2024-08-04T19:00:05.052+0100] {taskinstance.py:2306} INFO - Starting attempt 1 of 2
[2024-08-04T19:00:05.062+0100] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): jungle_scraper> on 2024-08-03 18:00:00+00:00
[2024-08-04T19:00:05.072+0100] {standard_task_runner.py:63} INFO - Started process 502652 to run task
[2024-08-04T19:00:05.076+0100] {standard_task_runner.py:90} INFO - Running: ['airflow', 'tasks', 'run', 'jobs_etl', 'jungle_scraper', 'scheduled__2024-08-03T18:00:00+00:00', '--job-id', '286', '--raw', '--subdir', 'DAGS_FOLDER/jobs_etl.py', '--cfg-path', '/tmp/tmp4lymdmn_']
[2024-08-04T19:00:05.079+0100] {standard_task_runner.py:91} INFO - Job 286: Subtask jungle_scraper
[2024-08-04T19:00:05.153+0100] {task_command.py:426} INFO - Running <TaskInstance: jobs_etl.jungle_scraper scheduled__2024-08-03T18:00:00+00:00 [running]> on host huser
[2024-08-04T19:00:05.265+0100] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_EMAIL='hassounibarka@gmail.com' AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='jobs_etl' AIRFLOW_CTX_TASK_ID='jungle_scraper' AIRFLOW_CTX_EXECUTION_DATE='2024-08-03T18:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-08-03T18:00:00+00:00'
[2024-08-04T19:00:05.266+0100] {taskinstance.py:430} INFO - ::endgroup::
[2024-08-04T19:00:20.156+0100] {logging_mixin.py:188} INFO - 2 Jobs for data Scientist
[2024-08-04T19:00:20.156+0100] {logging_mixin.py:188} INFO - data Scientist Job extraction ...
[2024-08-04T19:00:41.051+0100] {logging_mixin.py:188} INFO - 0 Jobs read1 Jobs read5 Jobs for data analyst
[2024-08-04T19:00:41.052+0100] {logging_mixin.py:188} INFO - data analyst Job extraction ...
[2024-08-04T19:01:15.330+0100] {logging_mixin.py:188} INFO - 0 Jobs read1 Jobs read2 Jobs read3 Jobs read4 Jobs read8 Jobs for data engineer
[2024-08-04T19:01:15.333+0100] {logging_mixin.py:188} INFO - data engineer Job extraction ...
[2024-08-04T19:02:07.707+0100] {logging_mixin.py:188} INFO - 0 Jobs read1 Jobs read2 Jobs read3 Jobs read4 Jobs read5 Jobs read6 Jobs read7 Jobs read21 Jobs for web developer
[2024-08-04T19:02:07.708+0100] {logging_mixin.py:188} INFO - web developer Job extraction ...
[2024-08-04T19:05:18.606+0100] {logging_mixin.py:188} INFO - 0 Jobs read1 Jobs read2 Jobs read3 Jobs read4 Jobs read5 Jobs read6 Jobs read7 Jobs read8 Jobs read9 Jobs read11 Jobs read12 Jobs read13 Jobs read14 Jobs read15 Jobs read16 Jobs read17 Jobs read18 Jobs read19 Jobs read20 Jobs read4 Jobs for mobile developer
[2024-08-04T19:05:18.607+0100] {logging_mixin.py:188} INFO - mobile developer Job extraction ...
[2024-08-04T19:05:35.429+0100] {logging_mixin.py:188} INFO - 0 Jobs read1 Jobs read2 Jobs read3 Jobs readDone
[2024-08-04T19:05:35.443+0100] {logging_mixin.py:188} INFO - Done!
[2024-08-04T19:05:35.444+0100] {python.py:237} INFO - Done. Returned value was: None
[2024-08-04T19:05:35.444+0100] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-08-04T19:05:35.459+0100] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=jobs_etl, task_id=jungle_scraper, run_id=scheduled__2024-08-03T18:00:00+00:00, execution_date=20240803T180000, start_date=20240804T180005, end_date=20240804T180535
[2024-08-04T19:05:35.485+0100] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2024-08-04T19:05:35.515+0100] {taskinstance.py:3498} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-08-04T19:05:35.516+0100] {local_task_job_runner.py:222} INFO - ::endgroup::
